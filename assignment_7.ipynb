{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+ltr9xEdVb0GMxkS2YR7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunami20/Data-Science-2025/blob/main/assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5W1ZqCm68-h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "17nyRXC47Giy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xmJ_a-W9Q6H",
        "outputId": "11b14c5f-8dc5-48d3-c1e1-31470118477a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/datasets/Housing.csv')\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFztdc0X-NrR",
        "outputId": "2e47fc0e-951e-4dde-80a1-393c07823dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
            "0  13300000  7420         4          2        3      yes        no       no   \n",
            "1  12250000  8960         4          4        4      yes        no       no   \n",
            "2  12250000  9960         3          2        2      yes        no      yes   \n",
            "3  12215000  7500         4          2        2      yes        no      yes   \n",
            "4  11410000  7420         4          1        2      yes       yes      yes   \n",
            "\n",
            "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
            "0              no             yes        2      yes        furnished  \n",
            "1              no             yes        3       no        furnished  \n",
            "2              no              no        2      yes   semi-furnished  \n",
            "3              no             yes        3      yes        furnished  \n",
            "4              no             yes        2       no        furnished  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Baasic exploration\n",
        "\n",
        "print('Dataset Shape: ', df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59QtByys-9_J",
        "outputId": "80540eb8-4909-4300-f0e5-5278c274b368"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape:  (545, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# information show\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB-Yo_Nv_OKj",
        "outputId": "7de23ef0-f583-42a0-fe4f-5d2ce6964c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 545 entries, 0 to 544\n",
            "Data columns (total 13 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   price             545 non-null    int64 \n",
            " 1   area              545 non-null    int64 \n",
            " 2   bedrooms          545 non-null    int64 \n",
            " 3   bathrooms         545 non-null    int64 \n",
            " 4   stories           545 non-null    int64 \n",
            " 5   mainroad          545 non-null    object\n",
            " 6   guestroom         545 non-null    object\n",
            " 7   basement          545 non-null    object\n",
            " 8   hotwaterheating   545 non-null    object\n",
            " 9   airconditioning   545 non-null    object\n",
            " 10  parking           545 non-null    int64 \n",
            " 11  prefarea          545 non-null    object\n",
            " 12  furnishingstatus  545 non-null    object\n",
            "dtypes: int64(6), object(7)\n",
            "memory usage: 55.5+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics\n",
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmo1DCC3_c6e",
        "outputId": "ce8df356-c057-4aaf-f0a6-eef5aa55d77a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              price          area    bedrooms   bathrooms     stories  \\\n",
            "count  5.450000e+02    545.000000  545.000000  545.000000  545.000000   \n",
            "mean   4.766729e+06   5150.541284    2.965138    1.286239    1.805505   \n",
            "std    1.870440e+06   2170.141023    0.738064    0.502470    0.867492   \n",
            "min    1.750000e+06   1650.000000    1.000000    1.000000    1.000000   \n",
            "25%    3.430000e+06   3600.000000    2.000000    1.000000    1.000000   \n",
            "50%    4.340000e+06   4600.000000    3.000000    1.000000    2.000000   \n",
            "75%    5.740000e+06   6360.000000    3.000000    2.000000    2.000000   \n",
            "max    1.330000e+07  16200.000000    6.000000    4.000000    4.000000   \n",
            "\n",
            "          parking  \n",
            "count  545.000000  \n",
            "mean     0.693578  \n",
            "std      0.861586  \n",
            "min      0.000000  \n",
            "25%      0.000000  \n",
            "50%      0.000000  \n",
            "75%      1.000000  \n",
            "max      3.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values Check\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY7IRGfd_zo4",
        "outputId": "31b47ef8-7463-4909-f770-cce94125dd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "price               0\n",
            "area                0\n",
            "bedrooms            0\n",
            "bathrooms           0\n",
            "stories             0\n",
            "mainroad            0\n",
            "guestroom           0\n",
            "basement            0\n",
            "hotwaterheating     0\n",
            "airconditioning     0\n",
            "parking             0\n",
            "prefarea            0\n",
            "furnishingstatus    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable distributaion\n",
        "\n",
        "print(df['furnishingstatus'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGb438ecAGT6",
        "outputId": "a6ea673b-bff8-4bb8-a656-010cc515b7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "furnishingstatus\n",
            "semi-furnished    227\n",
            "unfurnished       178\n",
            "furnished         140\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "76w5aW-8M2D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy df to avoid modifying original\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Identify categorical columns for X (excluding the target 'furnishingstatus')\n",
        "categorical_cols_for_X = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "# Apply one-hot encoding for these categorical columns in X\n",
        "# drop_first=True avoids multicollinearity\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols_for_X, drop_first=True, dtype=int)\n",
        "\n",
        "# Separate input (X) and output (Y)\n",
        "X = df_encoded.drop('furnishingstatus', axis=1)\n",
        "Y = df_encoded['furnishingstatus']\n",
        "\n",
        "# Encode the target variable Y\n",
        "label_encoder_y = LabelEncoder()\n",
        "Y = label_encoder_y.fit_transform(Y)\n",
        "\n",
        "print('X has all numerical input features (after encoding)')\n",
        "print('Y has encoded furnishingstatus (0, 1, or 2)')\n",
        "\n",
        "# Split: 80 % for trainning, 20% for testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size= 0.2,\n",
        "    random_state =20\n",
        ")\n",
        "\n",
        "print(f'We will train with: {len(X_train)} samples')\n",
        "print(f'We will test with: {len(X_test)} samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_RVGE3lUsL4",
        "outputId": "7020e0d8-917b-4acc-bf49-b46e7d56ab85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X has all patient informationn (8 comlumns)\n",
            "y has Housing yes/no (1 column)\n",
            " we will train with: 436 owner\n",
            " we will test with: 109 owner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **why scale **"
      ],
      "metadata": {
        "id": "lbdJVVdssnZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem: Some numbers are big, some are small\n",
        "\n",
        "print(\"Before scalling - different ranges:\")\n",
        "print(f\"Age: smallest={X_train['bathrooms'].min()}, biggest={X_train['bathrooms'].max()}\")\n",
        "print(f'Price: smallest={X_train['bedrooms'].min()}, biggest={X_train['bedrooms'].max()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXHrNePysrXD",
        "outputId": "9279385d-c97f-40ae-8e85-dd272dc663c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before scalling - different ranges:\n",
            "Age: smallest=1, biggest=4\n",
            "Price: smallest=1, biggest=6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fk6XBDoNwRiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **#Standdard Scaler**"
      ],
      "metadata": {
        "id": "5LehsQICwS0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creat scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Learn from training data and scale it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale test data (just transform, don't fit)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n After Scaling\")\n",
        "print(\" All features now have similar range!\")\n",
        "\n",
        "# Adjusted print statement for new column names after get_dummies\n",
        "# We'll check 'basement_yes' column which was originally 'basement'\n",
        "if 'basement_yes' in X.columns:\n",
        "    basement_col_idx = X.columns.get_loc('basement_yes')\n",
        "    print(f\"Example - First housing basement before scaling: {X_train.iloc[0][basement_col_idx]}\")\n",
        "    print(f\"Example - First housing basement after scaling: {X_train_scaled[0][basement_col_idx]}\")\n",
        "else:\n",
        "    print(\"Could not find 'basement_yes' column for example print.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6TEtUuVwX7B",
        "outputId": "f21bbdc6-ab51-499a-bcd9-4a129622fadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After Scaling\n",
            " All features now have similar range!\n",
            "Could not find 'basement_yes' column for example print.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKMCzHAiz1Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MinMaxScaler (Min-Max normalized)**"
      ],
      "metadata": {
        "id": "jKqL7z21z20i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Craete MinMaxscaler\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "# Learn from training data and scale it\n",
        "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale test data\n",
        "X_test_minmax =minmax_scaler.transform(X_test)\n",
        "\n",
        "print(\"\\n After MinMaxScaler:\")\n",
        "print(f\"Example - First housing age before scaling: {X_train.iloc[0][7]}\")\n",
        "print(f\"Example - First housing age after scaling: {X_train_minmax[0][7]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHG2Zuivz_Fc",
        "outputId": "43a3f7e5-6ef5-40fc-f61b-dd05d7f37bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After MinMaxScaler:\n",
            "Example - First housing age before scaling: nan\n",
            "Example - First housing age after scaling: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOwEe_EF1shE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoding Categorical Data**"
      ],
      "metadata": {
        "id": "waQTG_BP1tjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a sample dataset with text data\n",
        "sample_data = pd.DataFrame({\n",
        "    'age': [23, 26, 35, 40],\n",
        "    'buyer': ['male', 'female', 'male', 'female'],\n",
        "    'agree': ['yes', 'no', 'yes', 'no'],\n",
        "    'problem': [1,0,1,0]\n",
        "})\n",
        "\n",
        "print(sample_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPv3qkiO1y3J",
        "outputId": "2516af7e-0678-4495-e00f-20d00e0f724c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age   buyer agree  problem\n",
            "0   23    male   yes        1\n",
            "1   26  female    no        0\n",
            "2   35    male   yes        1\n",
            "3   40  female    no        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Label Encoding (for Binary/Two categories)**"
      ],
      "metadata": {
        "id": "ha7-_79021vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# For yes? no or male /female\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# convert agree colum\n",
        "sample_data['agree_encoded'] = label_encoder.fit_transform(sample_data['agree'])\n",
        "print(\"\\n After Label Encoding 'agree' :\")\n",
        "print(sample_data[['agree', 'agree_encoded']])\n",
        "\n",
        "# convert 'buyer' column (corrected from 'gender')\n",
        "sample_data['gender_encoded'] = label_encoder.fit_transform(sample_data['buyer'])\n",
        "print(\"\\nAfter Label Encoding 'buyer':\")\n",
        "print(sample_data[['buyer', 'gender_encoded']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRjrvQFi2_Qf",
        "outputId": "a184e3a3-ae6d-4d4c-db83-6f69a5c72ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After Label Encoding 'agree' :\n",
            "  agree  agree_encoded\n",
            "0   yes              1\n",
            "1    no              0\n",
            "2   yes              1\n",
            "3    no              0\n",
            "\n",
            "After Label Encoding 'buyer':\n",
            "    buyer  gender_encoded\n",
            "0    male               1\n",
            "1  female               0\n",
            "2    male               1\n",
            "3  female               0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One Hot Encoding**"
      ],
      "metadata": {
        "id": "CJ6Fouvp_Kuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let;s add a column with multiple categories\n",
        "sample_data['city'] =['Delhi', 'Mumbai', 'Delhi', 'Kolkata']\n",
        "\n",
        "# One-Hot Encoding using pandas\n",
        "city_encoded = pd.get_dummies(sample_data['city'], prefix='city')\n",
        "print(\"n One-Hot Encoding using for city\" )\n",
        "print(city_encoded)\n",
        "\n",
        "# Creates separete columns for each city\n",
        "\n",
        "# Add to original data\n",
        "sample_data = pd.concat([sample_data, city_encoded], axis=1)\n",
        "print('\\n Final data with all encoding: ')\n",
        "print(sample_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emyTg57L_OoJ",
        "outputId": "1d8229d4-c0d0-4160-e835-d4e9d5e79acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n One-Hot Encoding using for city\n",
            "   city_Delhi  city_Kolkata  city_Mumbai\n",
            "0        True         False        False\n",
            "1       False         False         True\n",
            "2        True         False        False\n",
            "3       False          True        False\n",
            "\n",
            " Final data with all encoding: \n",
            "   age   buyer agree  problem  agree_encoded  gender_encoded     city  \\\n",
            "0   23    male   yes        1              1               1    Delhi   \n",
            "1   26  female    no        0              0               0   Mumbai   \n",
            "2   35    male   yes        1              1               1    Delhi   \n",
            "3   40  female    no        0              0               0  Kolkata   \n",
            "\n",
            "   city_Delhi  city_Kolkata  city_Mumbai  \n",
            "0        True         False        False  \n",
            "1       False         False         True  \n",
            "2        True         False        False  \n",
            "3       False          True        False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T-i6gDz0C3wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train a Simple Model**"
      ],
      "metadata": {
        "id": "hnAFwAPuKato"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, Y_train)\n",
        "print(\" Model trained!\")\n",
        "\n",
        "# check accuracy on training data\n",
        "train_score = model.score(X_train_scaled, Y_train)\n",
        "print(f\"Tranining accuracy: {train_score:.1%}\")\n",
        "\n",
        "# Check accuracy on test data\n",
        "test_score = model.score(X_test_scaled,Y_test)\n",
        "print(f\"Test accuracy: {test_score:.1%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "E-T7m9ToNVLp",
        "outputId": "30d896f7-9dae-4c05-8558-e91cefd0ba1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3847233820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Model trained!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m   1223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross - validation**"
      ],
      "metadata": {
        "id": "vXbzdR8BOkRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Do 5-fold cross-vlaidation\n",
        "scores = cross_val_score(\n",
        "    model,\n",
        "    X_train_scaled,\n",
        "    Y_train,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "print(\"5 mini-test scores\")\n",
        "for i in range(5):\n",
        "  print(f\" test {i+1}: {scores[i]:.1%}\")\n",
        "  print(f\"n Average: {scores.mean():1%}\")\n",
        "  print(f\"This means our model is {scores.mean():1%} accurate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cD2xZf76OpDd",
        "outputId": "a9bd558a-64a6-40d8-bd6f-15403a26ec96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 1222, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3523594849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Do 5-fold cross-vlaidation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m scores = cross_val_score(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# For callable scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\", line 1222, in fit\n    X, y = validate_data(\n           ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 2961, in validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1370, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1107, in check_array\n    _assert_all_finite(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 120, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 169, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
          ]
        }
      ]
    }
  ]
}